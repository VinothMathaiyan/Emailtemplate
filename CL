
text‑analytics challenge in our On‑boarding Journey programme and to ask for your support in building a more granular topic‑and‑keyword model.

1 | Current Situation
We run parallel Qualtrics surveys for Hiring Managers (HM) and New Joiners (NJ).

Each journey has 5‑7 stages (e.g., HM = “Approval Experience”, “Interviewing”, “Day 1 Welcome”; NJ = “Application Experience”, “Interview Experience”, “On‑boarding Apps”, “Day 1 Experience”).

Every stage contains at least one free‑text question. Volumes now exceed the point where manual reading is feasible.

2 | Challenge with the Existing n‑gram Output
Your earlier unigram / bigram / trigram list correctly surfaces the highest‑frequency terms (“candidate”, “interview”, “process”, “system”…).
However those same words appear in all stages, so when we tag responses the result is flat and non‑diagnostic; stakeholders cannot see which pain‑points are specific to each stage.

3 | Business Need
We need a model that, for each stage, produces:

Level	Example – HM ‘Approval Experience’	Example – NJ ‘Application Experience’
Theme	Approval efficiency / delays	JD clarity & system usability
Topic	“Slow approval turnaround” / “Confusing authority chain”	“Job description unclear” / “ATS timeout”
Keywords*	approval + delay, approval + slow, “long time”, “stuck queue”	JD + unclear, application + confusing, system + error

* Conjunctions / collocations (bigrams / trigrams) preferred so that automatic tagging is stage‑specific.

4 | Request
Stage‑aware Topic Modelling – Identify 3‑5 themes per stage (HM and NJ separately) with supporting topic names.

Keyword Dictionary – For each topic supply the collocated bigrams / trigrams that trigger the tag.

Neutral Framing – Keywords should be sentiment‑agnostic; TextIQ will later split sentiment into +/‑/neutral.

Deliverable Format – CSV or JSON lookup table we can import into TextIQ / XM Discover.

5 | Why It Matters
Enables stakeholders to see, at a glance, which stage is causing delays, confusion, or delight.

Reduces manual tagging effort and drives faster, data‑driven improvements to the on‑boarding journey.

6 | Next Steps
If you are able to take this on, could we set up a brief scoping call early next week?
Happy to provide:

Sample anonymised transcripts (per stage)

Current n‑gram output for reference

Volume statistics & required timelines

Thanks in advance for your help—your expertise will make a measurable difference to how quickly we can act on colleague feedback.



-------------------

Research Goal
Understand the practices, workflows, and governance that allow Team X to derive stage‑specific, actionable insights from XM Discover, and identify which of those practices can be transferred to the Hiring‑Manager / New‑Joiner journey.

Method: qualitative, semi‑structured interview (60 min); follow‑up artefact review (taxonomy documents, dashboards, codebooks).

Section	Purpose	Core Questions (✓ open) → Follow‑up Probes
1. Framing & Outcomes-	Clarify their business context and success criteria.	✓ What journey(s) or programmes do you analyse with XM Discover?
→ Which metrics or decisions depend most on the insights?
→ How do you define “success” for text‑analytics outputs?
2. Data Preparation-	Understand how raw responses are cleaned and segmented before Discover ingest.	✓ Can you walk us through your pipeline from survey close → dataset in XM Discover?
→ Do you filter or enrich records by stage, persona, or metadata?
→ Any automated PII stripping or language normalisation?
3. Taxonomy / Theme Design-	Learn how they arrived at useful themes, topics, sub‑topics.	✓ Did you start with Discover’s auto‑suggested topics or build a custom taxonomy?
→ Who owns taxonomy design (researcher vs. business SME)?
→ How do you ensure topics are distinct across stages?
→ How often do you revisit / refine the taxonomy?
4. Keyword & Phrase Strategy-	Capture tactics for generating meaningful n‑grams / collocations per stage.	✓ How do you generate and select the final keyword lists for each topic?
→ Do you rely on Discover n‑gram counts, separate NLP scripts, or SME curation?
→ Any heuristics for excluding “noise” words that appear in every stage?
→ Do you weight keywords or employ proximity rules (e.g., “approval” within 5 words of “delay”)?
5. Sentiment & Signal Extraction-	See how they separate neutral themes from sentiment‑bearing content.	✓ Do you let Discover’s native sentiment model tag polarity, or do you overlay custom rules?
→ If a theme spans both positive and negative sentiment, how do you surface that split to stakeholders?
6. Validation & Accuracy-	Discover how they test that topics really reflect the underlying text.	✓ How do you validate that a response is mapped to the right theme?
→ Manual sample review? Precision / recall metrics?
→ How often do you run QA cycles?
7. Integration & Consumption-	Learn how outputs feed dashboards or downstream systems.	✓ Where do the Discover insights go (dashboards, PPTs, BI tools)?
→ Any automated API exports / schedule?
→ How do hiring managers / HR teams act on the insights?
8. Governance & Access-	Capture controls that keep sensitive text secure.	✓ Who has analyst vs. read‑only access in Discover?
→ What masking or redaction policies are enforced?
→ Incident process if sensitive info slips through?
9. Lessons & Transferability-	Directly elicit tips relevant to our HM/NJ use‑case.	✓ If you were starting over, what would you do differently?
→ Which practices do you think would translate well to a multi‑stage HM/NJ journey?
→ What pitfalls should we avoid?
10. Wrap‑Up	- Next steps & artefact sharing.	✓ Could you share sample taxonomies or keyword dictionaries (with any sensitive data removed)?
→ Would a short follow‑up workshop be possible once we trial changes?


Use these questions as your master list—you can drop or reorder items depending on time and the interviewee’s expertise. This structure should surface 
exactly how Team X extracts stage‑specific themes and what we need to adopt in our own XM Discover implementation.


-------

In the Hiring‑Manager / New‑Joiner journey we collect ~30 k open‑text responses a year.
Using XM Discover we currently see high‑frequency, generic words (‘candidate’, ‘process’, ‘system’) across all stages, but few stage‑specific themes.
We want to understand how your Textile team has configured XM Discover to surface actionable, stage‑specific insights, including any external tooling or workflows you use to generate keywords and taxonomies.

1 | Use‑case & Success Metrics
Q1.1 Could you describe the business question(s) your Textile team answers with XM Discover?
Q1.2 What KPI or outcome tells you the analysis is “working”?

Probes: volume of tickets analysed, % manual review eliminated, time‑to‑insight.

2 | Data Ingestion & Pre‑Processing
Q2.1 How do you get raw text into XM Discover? API, CSV batch, or other?
Q2.2 Do you clean or enrich the text (e.g., remove PII, add stage labels) before ingestion?

Probes: language detection, stemming / lemmatization, metadata mapping.

3 | Theme / Topic Development
Q3.1 Did you start with Discover’s auto‑generated topics or design your own taxonomy?
Q3.2 What were the key steps—from first dump of n‑grams to the final, named themes?
Q3.3 If you built a custom taxonomy:
  a) Who owned the process (analyst vs. business SME)?
  b) How many iterations did it take before stakeholders were satisfied?

Probes: concept merging criteria, documentation of definitions, version control.

4 | Keyword / Phrase Strategy
Q4.1 How do you decide which unigrams / bigrams / trigrams become “keywords” for each theme?
Q4.2 Do you use external NLP tools (Python, R, AutoML) to generate candidate phrases?
Q4.3 Are you applying any rules like proximity, negation handling or stop‑word whitelists?

Probes: threshold for term frequency, manual SME curation, handling synonyms.

5 | Automation & Maintenance
Q5.1 How often do you rebuild or refresh themes and keyword lists?
Q5.2 What triggers a refresh—volume growth, business change, model drift?
Q5.3 Do you have scripts / pipelines that automate any of the rebuild steps?

Probes: CI/CD for taxonomy files, scheduled jobs, approval workflow.

6 | Validation, Visualisation & Action
Q6.1 How do you validate that a response is correctly tagged? (precision / recall, manual QA, stakeholder sign‑offs?)
Q6.2 Which Discover visualisations or external dashboards do your stakeholders rely on most?
Q6.3 Can you share an example of a decision or change that was driven by Discover insights?

Probes: escalation routes for critical sentiment, integration with Power BI / Tableau, feedback loops.

Closing Requests
Artefacts — Could you share (sanitised) examples of your taxonomy, keyword dictionaries or pipeline scripts?

Follow‑up — Would you be open to a short workshop once we prototype a similar approach in HR?

How to Use
Pick & prioritise blocks based on the 60 min you have; start with Sections 1‑4, then dive deeper if time allows.

Capture quotations verbatim; note any tooling references you can test later.

Return the favour by sharing your own lessons once your HM/NJ model improves—collaboration builds goodwill.


--------



Problem Context (you can share this when reaching out)
We’re using Qualtrics XM Discover to analyze ~30,000 free-text responses collected from both Hiring Managers and New Joiners during various onboarding stages (e.g., application, approval, interview, Day 1).

However, we are struggling to generate meaningful, stage-specific themes or insights. The keywords we've derived (e.g., candidate, interview, process) appear too frequently across all stages, offering little differentiation.

Since XM Discover requires us to supply themes and keywords manually, we’re trying to understand:

How your Technology team has successfully operationalized XM Discover

What workflow, tools, or methodology you follow to design a meaningful keyword/topic structure

Whether you're combining Discover with any external NLP tools or data science workflows

📋 Interview Guide: Research Questions for Technology Team
1. Use Case & Purpose
What kind of problems or business questions are you trying to solve using XM Discover?

In your view, what does “success” look like when using XM Discover? Is it insight generation, faster decision-making, automated tagging, or something else?

2. Theme & Keyword Strategy
Since Discover doesn't auto-generate topics, how do you create your initial list of themes?

Who decides what themes to include?

How do you decide which keywords or phrases belong to which themes?

Are you using data science tools like Python/R to extract these from historical text?

How do you avoid overlaps across stages?

Do you use keyword combinations (e.g., "approval" + "delay", "system" + "not working") to improve accuracy?

3. Stage-Specific Relevance
How do you ensure that the keywords/themes are relevant to specific stages or journeys (e.g., application, approval, interview)?

Do you add metadata (like stage name) to the text input?

Is Discover able to show differences in sentiment or topics per stage?

4. External Tools & Workflow
Do you use external tools (e.g., Excel, NLP libraries, GenAI, etc.) to prepare:

High-frequency n-grams?

Topic modeling (e.g., LDA, BERTopic)?

Sentiment or emotion classification?

Do you have any semi-automated pipelines for keyword list generation or update?

5. Team Setup & Collaboration
Who is responsible for the Discover setup? Is it a data analyst, a developer, or someone from the business team?

How often do you revisit or revise the themes and keywords? What triggers a change?

6. Visualization & Usage
How do you use the results from Discover?

Do you export the topic tags to another dashboard or tool?

Are you applying these insights directly to influence business decisions?

Have you created any manual QA process to ensure theme-tagging is accurate?

7. Lessons Learned & Best Practices
What were your early mistakes or pitfalls when starting with Discover?

What would you recommend we definitely do or avoid doing when setting up themes and keywords for our use case?

📌 Final Request
Would you be open to reviewing a small sample of our current themes and keyword mappings to give us feedback on how we might improve?

This approach shows that:

You understand the limits of the tool (manual theme/keyword input)

You're trying to learn their strategy, not just copy results

You’re interested in adapting their approach to your hiring/new joiner context, which has its own complexity (dual personas, multiple stages)

Let me know if you’d like me to turn this into an email or slide deck for stakeholder sharing.
