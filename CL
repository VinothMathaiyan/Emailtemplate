
textâ€‘analytics challenge in our Onâ€‘boarding Journey programme and to ask for your support in building a more granular topicâ€‘andâ€‘keyword model.

1â€¯|â€¯Current Situation
We run parallel Qualtrics surveys for Hiring Managers (HM) and Newâ€¯Joiners (NJ).

Each journey has 5â€‘7 stages (e.g., HMâ€¯=â€¯â€œApprovalâ€¯Experienceâ€, â€œInterviewingâ€, â€œDayâ€¯1â€¯Welcomeâ€; NJâ€¯=â€¯â€œApplicationâ€¯Experienceâ€, â€œInterviewâ€¯Experienceâ€, â€œOnâ€‘boardingâ€¯Appsâ€, â€œDayâ€¯1â€¯Experienceâ€).

Every stage contains at least one freeâ€‘text question. Volumes now exceed the point where manual reading is feasible.

2â€¯|â€¯Challenge with the Existing nâ€‘gram Output
Your earlier unigram / bigram / trigram list correctly surfaces the highestâ€‘frequency terms (â€œcandidateâ€, â€œinterviewâ€, â€œprocessâ€, â€œsystemâ€â€¦).
However those same words appear in all stages, so when we tag responses the result is flat and nonâ€‘diagnostic; stakeholders cannot see which painâ€‘points are specific to each stage.

3â€¯|â€¯Business Need
We need a model that, for each stage, produces:

Level	Example â€“ HM â€˜Approvalâ€¯Experienceâ€™	Example â€“ NJ â€˜Applicationâ€¯Experienceâ€™
Theme	Approvalâ€¯efficiency / delays	JD clarity & system usability
Topic	â€œSlow approval turnaroundâ€â€¯/ â€œConfusing authority chainâ€	â€œJob description unclearâ€â€¯/ â€œATS timeoutâ€
Keywords*	approvalâ€¯+â€¯delay, approvalâ€¯+â€¯slow, â€œlong timeâ€, â€œstuck queueâ€	JDâ€¯+â€¯unclear, applicationâ€¯+â€¯confusing, systemâ€¯+â€¯error

* Conjunctions / collocations (bigramsâ€¯/â€¯trigrams) preferred so that automatic tagging is stageâ€‘specific.

4â€¯|â€¯Request
Stageâ€‘aware Topic Modelling â€“ Identify 3â€‘5 themes per stage (HMâ€¯andâ€¯NJ separately) with supporting topic names.

Keyword Dictionary â€“ For each topic supply the collocated bigrams / trigrams that trigger the tag.

Neutral Framing â€“ Keywords should be sentimentâ€‘agnostic; TextIQ will later split sentiment intoâ€¯+/â€‘/neutral.

Deliverable Format â€“ CSV or JSON lookup table we can import into TextIQ / XMâ€¯Discover.

5â€¯|â€¯Why It Matters
Enables stakeholders to see, at a glance, which stage is causing delays, confusion, or delight.

Reduces manual tagging effort and drives faster, dataâ€‘driven improvements to the onâ€‘boarding journey.

6â€¯|â€¯Next Steps
If you are able to take this on, could we set up a brief scoping call early next week?
Happy to provide:

Sample anonymised transcripts (per stage)

Current nâ€‘gram output for reference

Volume statistics & required timelines

Thanks in advance for your helpâ€”your expertise will make a measurable difference to how quickly we can act on colleague feedback.



-------------------

Research Goal
Understand the practices, workflows, and governance that allow Teamâ€¯X to derive stageâ€‘specific, actionable insights from XMâ€¯Discover, and identify which of those practices can be transferred to the Hiringâ€‘Manager / Newâ€‘Joiner journey.

Method: qualitative, semiâ€‘structured interview (60â€¯min); followâ€‘up artefact review (taxonomy documents, dashboards, codebooks).

Section	Purpose	Core Questions (âœ“Â open)Â â†’Â Followâ€‘up Probes
1. Framing & Outcomes-	Clarify their business context and success criteria.	âœ“â€¯What journey(s) or programmes do you analyse with XMâ€¯Discover?
â†’â€¯Which metrics or decisions depend most on the insights?
â†’â€¯How do you define â€œsuccessâ€ for textâ€‘analytics outputs?
2. Data Preparation-	Understand how raw responses are cleaned and segmented before Discover ingest.	âœ“â€¯Can you walk us through your pipeline from survey close â†’ dataset in XMâ€¯Discover?
â†’â€¯Do you filter or enrich records by stage, persona, or metadata?
â†’â€¯Any automated PII stripping or language normalisation?
3. Taxonomy / Theme Design-	Learn how they arrived at useful themes, topics, subâ€‘topics.	âœ“â€¯Did you start with Discoverâ€™s autoâ€‘suggested topics or build a custom taxonomy?
â†’â€¯Who owns taxonomy design (researcher vs. business SME)?
â†’â€¯How do you ensure topics are distinct across stages?
â†’â€¯How often do you revisit / refine the taxonomy?
4. Keyword & Phrase Strategy-	Capture tactics for generating meaningful nâ€‘grams / collocations per stage.	âœ“â€¯How do you generate and select the final keyword lists for each topic?
â†’â€¯Do you rely on Discover nâ€‘gram counts, separate NLP scripts, or SME curation?
â†’â€¯Any heuristics for excluding â€œnoiseâ€ words that appear in every stage?
â†’â€¯Do you weight keywords or employ proximity rules (e.g., â€œapprovalâ€ withinâ€¯5â€¯words of â€œdelayâ€)?
5. Sentiment & Signal Extraction-	See how they separate neutral themes from sentimentâ€‘bearing content.	âœ“â€¯Do you let Discoverâ€™s native sentiment model tag polarity, or do you overlay custom rules?
â†’â€¯If a theme spans both positive and negative sentiment, how do you surface that split to stakeholders?
6. Validation & Accuracy-	Discover how they test that topics really reflect the underlying text.	âœ“â€¯How do you validate that a response is mapped to the right theme?
â†’â€¯Manual sample review? Precision / recall metrics?
â†’â€¯How often do you run QA cycles?
7. Integration & Consumption-	Learn how outputs feed dashboards or downstream systems.	âœ“â€¯Where do the Discover insights go (dashboards, PPTs, BI tools)?
â†’â€¯Any automated API exports / schedule?
â†’â€¯How do hiring managers / HR teams act on the insights?
8. Governance & Access-	Capture controls that keep sensitive text secure.	âœ“â€¯Who has analyst vs. readâ€‘only access in Discover?
â†’â€¯What masking or redaction policies are enforced?
â†’â€¯Incident process if sensitive info slips through?
9. Lessons & Transferability-	Directly elicit tips relevant to our HM/NJ useâ€‘case.	âœ“â€¯If you were starting over, what would you do differently?
â†’â€¯Which practices do you think would translate well to a multiâ€‘stage HM/NJ journey?
â†’â€¯What pitfalls should we avoid?
10. Wrapâ€‘Up	- Next steps & artefact sharing.	âœ“â€¯Could you share sample taxonomies or keyword dictionaries (with any sensitive data removed)?
â†’â€¯Would a short followâ€‘up workshop be possible once we trial changes?


Use these questions as your master listâ€”you can drop or reorder items depending on time and the intervieweeâ€™s expertise. This structure should surface 
exactly how Teamâ€¯X extracts stageâ€‘specific themes and what we need to adopt in our own XMâ€¯Discover implementation.


-------

In the Hiringâ€‘Manager / Newâ€‘Joiner journey we collectâ€¯~30â€¯k openâ€‘text responses a year.
Using XMâ€¯Discover we currently see highâ€‘frequency, generic words (â€˜candidateâ€™, â€˜processâ€™, â€˜systemâ€™) across all stages, but few stageâ€‘specific themes.
We want to understand how your Textile team has configured XMâ€¯Discover to surface actionable, stageâ€‘specific insights, including any external tooling or workflows you use to generate keywords and taxonomies.

1â€¯|â€¯Useâ€‘case & Success Metrics
Q1.1â€¯Could you describe the business question(s) your Textile team answers with XMâ€¯Discover?
Q1.2â€¯What KPI or outcome tells you the analysis is â€œworkingâ€?

Probes: volume of tickets analysed, % manual review eliminated, timeâ€‘toâ€‘insight.

2â€¯|â€¯Data Ingestion & Preâ€‘Processing
Q2.1â€¯How do you get raw text into XMâ€¯Discover? API, CSV batch, or other?
Q2.2â€¯Do you clean or enrich the text (e.g., remove PII, add stage labels) before ingestion?

Probes: language detection, stemming / lemmatization, metadata mapping.

3â€¯|â€¯Theme / Topic Development
Q3.1â€¯Did you start with Discoverâ€™s autoâ€‘generated topics or design your own taxonomy?
Q3.2â€¯What were the key stepsâ€”from first dump of nâ€‘grams to the final, named themes?
Q3.3â€¯If you built a custom taxonomy:
Â Â a)â€¯Who owned the process (analyst vs. business SME)?
Â Â b)â€¯How many iterations did it take before stakeholders were satisfied?

Probes: concept merging criteria, documentation of definitions, version control.

4â€¯|â€¯Keyword / Phrase Strategy
Q4.1â€¯How do you decide which unigrams / bigrams / trigrams become â€œkeywordsâ€ for each theme?
Q4.2â€¯Do you use external NLP tools (Python, R, AutoML) to generate candidate phrases?
Q4.3â€¯Are you applying any rules like proximity, negation handling or stopâ€‘word whitelists?

Probes: threshold for term frequency, manual SME curation, handling synonyms.

5â€¯|â€¯Automation & Maintenance
Q5.1â€¯How often do you rebuild or refresh themes and keyword lists?
Q5.2â€¯What triggers a refreshâ€”volume growth, business change, model drift?
Q5.3â€¯Do you have scripts / pipelines that automate any of the rebuild steps?

Probes: CI/CD for taxonomy files, scheduled jobs, approval workflow.

6â€¯|â€¯Validation, Visualisation & Action
Q6.1â€¯How do you validate that a response is correctly tagged? (precision / recall, manual QA, stakeholder signâ€‘offs?)
Q6.2â€¯Which Discover visualisations or external dashboards do your stakeholders rely on most?
Q6.3â€¯Can you share an example of a decision or change that was driven by Discover insights?

Probes: escalation routes for critical sentiment, integration with Powerâ€¯BI / Tableau, feedback loops.

Closing Requests
Artefactsâ€¯â€” Could you share (sanitised) examples of your taxonomy, keyword dictionaries or pipeline scripts?

Followâ€‘upâ€¯â€” Would you be open to a short workshop once we prototype a similar approach in HR?

How to Use
Pick & prioritise blocks based on the 60â€¯min you have; start with Sectionsâ€¯1â€‘4, then dive deeper if time allows.

Capture quotations verbatim; note any tooling references you can test later.

Return the favour by sharing your own lessons once your HM/NJ model improvesâ€”collaboration builds goodwill.


--------



Problem Context (you can share this when reaching out)
Weâ€™re using Qualtrics XM Discover to analyze ~30,000 free-text responses collected from both Hiring Managers and New Joiners during various onboarding stages (e.g., application, approval, interview, Day 1).

However, we are struggling to generate meaningful, stage-specific themes or insights. The keywords we've derived (e.g., candidate, interview, process) appear too frequently across all stages, offering little differentiation.

Since XM Discover requires us to supply themes and keywords manually, weâ€™re trying to understand:

How your Technology team has successfully operationalized XM Discover

What workflow, tools, or methodology you follow to design a meaningful keyword/topic structure

Whether you're combining Discover with any external NLP tools or data science workflows

ğŸ“‹ Interview Guide: Research Questions for Technology Team
1. Use Case & Purpose
What kind of problems or business questions are you trying to solve using XM Discover?

In your view, what does â€œsuccessâ€ look like when using XM Discover? Is it insight generation, faster decision-making, automated tagging, or something else?

2. Theme & Keyword Strategy
Since Discover doesn't auto-generate topics, how do you create your initial list of themes?

Who decides what themes to include?

How do you decide which keywords or phrases belong to which themes?

Are you using data science tools like Python/R to extract these from historical text?

How do you avoid overlaps across stages?

Do you use keyword combinations (e.g., "approval" + "delay", "system" + "not working") to improve accuracy?

3. Stage-Specific Relevance
How do you ensure that the keywords/themes are relevant to specific stages or journeys (e.g., application, approval, interview)?

Do you add metadata (like stage name) to the text input?

Is Discover able to show differences in sentiment or topics per stage?

4. External Tools & Workflow
Do you use external tools (e.g., Excel, NLP libraries, GenAI, etc.) to prepare:

High-frequency n-grams?

Topic modeling (e.g., LDA, BERTopic)?

Sentiment or emotion classification?

Do you have any semi-automated pipelines for keyword list generation or update?

5. Team Setup & Collaboration
Who is responsible for the Discover setup? Is it a data analyst, a developer, or someone from the business team?

How often do you revisit or revise the themes and keywords? What triggers a change?

6. Visualization & Usage
How do you use the results from Discover?

Do you export the topic tags to another dashboard or tool?

Are you applying these insights directly to influence business decisions?

Have you created any manual QA process to ensure theme-tagging is accurate?

7. Lessons Learned & Best Practices
What were your early mistakes or pitfalls when starting with Discover?

What would you recommend we definitely do or avoid doing when setting up themes and keywords for our use case?

ğŸ“Œ Final Request
Would you be open to reviewing a small sample of our current themes and keyword mappings to give us feedback on how we might improve?

This approach shows that:

You understand the limits of the tool (manual theme/keyword input)

You're trying to learn their strategy, not just copy results

Youâ€™re interested in adapting their approach to your hiring/new joiner context, which has its own complexity (dual personas, multiple stages)

Let me know if youâ€™d like me to turn this into an email or slide deck for stakeholder sharing.
